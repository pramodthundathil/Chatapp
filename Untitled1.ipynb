{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70173531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aca73fc2634e89bd7e100ad8ae5038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e64353eae8645b5802fb9525c29f183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4352976cc4250901eeba1d9a81efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)uct-v0.1.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Digital\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\Users\\Digital\\AppData\\Roaming\\Python\\Python311\\site-packages\\ctransformers\\lib\\cuda\\ctransformers.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mctransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Mistral-7B-Instruct-v0.1-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-7b-instruct-v0.1.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI is going to\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ctransformers\\hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[1;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[0;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[0;32m    170\u001b[0m         model_file,\n\u001b[0;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    173\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(\n\u001b[0;32m    176\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m    177\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[0;32m    178\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    179\u001b[0m     lib\u001b[38;5;241m=\u001b[39mlib,\n\u001b[0;32m    180\u001b[0m )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ctransformers\\llm.py:246\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[1;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to detect model type. Please specify a model type using:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  AutoModelForCausalLM.from_pretrained(..., model_type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         )\n\u001b[0;32m    244\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m load_library(lib, gpu\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgpu_layers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[0;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ctransformers\\llm.py:126\u001b[0m, in \u001b[0;36mload_library\u001b[1;34m(path, gpu)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[0;32m    125\u001b[0m     load_cuda()\n\u001b[1;32m--> 126\u001b[0m lib \u001b[38;5;241m=\u001b[39m CDLL(path)\n\u001b[0;32m    128\u001b[0m lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    129\u001b[0m     c_char_p,  \u001b[38;5;66;03m# model_path\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     c_char_p,  \u001b[38;5;66;03m# model_type\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     ConfigStruct,  \u001b[38;5;66;03m# config\u001b[39;00m\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m llm_p\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ctypes\\__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Users\\Digital\\AppData\\Roaming\\Python\\Python311\\site-packages\\ctransformers\\lib\\cuda\\ctransformers.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "# Source: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87e03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting ctransformers\n",
      "  Obtaining dependency information for ctransformers from https://files.pythonhosted.org/packages/14/50/0b608e2abee4fc695b4e7ff5f569f5d32faf84a49e322034716fa157d1cf/ctransformers-0.2.27-py3-none-any.whl.metadata\n",
      "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\programdata\\anaconda3\\lib\\site-packages (from ctransformers) (0.15.1)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->ctransformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\digital\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->ctransformers) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->ctransformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->ctransformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\digital\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->ctransformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\digital\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->ctransformers) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->ctransformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->ctransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->ctransformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->ctransformers) (2023.7.22)\n",
      "Using cached ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "Installing collected packages: py-cpuinfo, ctransformers\n",
      "Successfully installed ctransformers-0.2.27 py-cpuinfo-9.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script cpuinfo.exe is installed in 'C:\\Users\\Digital\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "async def callback(contents: str):\n",
    "    llms = {}\n",
    "\n",
    "Â Â Â Â if \"mistral\" not in llms:\n",
    "Â Â Â Â Â Â Â Â llms[\"mistral\"] = AutoModelForCausalLM.from_pretrained(\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â gpu_layers=1,\n",
    "Â Â Â Â Â Â Â Â )\n",
    "\n",
    "Â Â Â Â llm = llms[\"mistral\"]\n",
    "Â Â Â Â response = llm(contents, stream=True, max_new_tokens=1000)\n",
    "Â Â Â Â message = \"\"\n",
    "\n",
    "Â Â Â Â for token in response:\n",
    "Â Â Â Â Â Â Â Â message += token\n",
    "Â Â Â Â Â Â Â Â yield message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65244faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demonstrates how to use the ChatInterface widget to create a chatbot using\n",
    "\n",
    "Mistral thru CTransformers.\n",
    "\n",
    "\"\"\"\n",
    "import panel as pn\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "\n",
    "Â Â Â Â if \"mistral\" not in llms:\n",
    "Â Â Â Â Â Â Â Â instance.placeholder_text = \"Downloading model; please wait...\"\n",
    "\n",
    "Â Â Â Â Â Â Â Â llms[\"mistral\"] = AutoModelForCausalLM.from_pretrained(\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â gpu_layers=1,\n",
    "Â Â Â Â Â Â Â Â )\n",
    "\n",
    "Â Â Â Â llm = llms[\"mistral\"]\n",
    "Â Â Â Â response = llm(contents, stream=True, max_new_tokens=1000)\n",
    "Â Â Â Â message = \"\"\n",
    "\n",
    "Â Â Â Â for token in response:\n",
    "Â Â Â Â Â Â Â Â message += token\n",
    "Â Â Â Â Â Â Â Â yield message\n",
    "\n",
    "llms = {}\n",
    "\n",
    "chat_interface = pn.chat.ChatInterface(callback=callback, callback_user=\"Mistral\")\n",
    "\n",
    "chat_interface.send(\n",
    "Â Â Â Â \"Send a message to get a reply from Mistral!\", user=\"System\", respond=False\n",
    ")\n",
    "\n",
    "chat_interface.servable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
