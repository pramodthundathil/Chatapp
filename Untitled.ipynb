{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae82001",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.core.query_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_bundle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryBundle\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseQueryEngine\n\u001b[0;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(stream\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.core.query_engine'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.query_engine.query_bundle import QueryBundle\n",
    "from llama_index.core.query_engine.base_query_engine import BaseQueryEngine\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "custom_cache_folder = \"C:/Users/Digital/.cache/huggingface/hub/models--thenlper--gte-large/snapshots/8cb729e8b44d9ec9d85c1cec4167ed28b43b04c2/1_Pooling/\"\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"Data\").load_data()\n",
    "\n",
    "# Create LlamaCPP instance\n",
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Create LangchainEmbedding instance\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\", cache_folder=custom_cache_folder))\n",
    "\n",
    "# Create ServiceContext\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=llm, embed_model=embed_model)\n",
    "\n",
    "# Create VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "# Create query engine from loaded index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the loaded index\n",
    "response = query_engine.query(\"What is 67?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1aae25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.query_engine.query_bundle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_bundle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryBundle\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseQueryEngine\n\u001b[0;32m      9\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(stream\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.query_engine.query_bundle'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.query_engine.query_bundle import QueryBundle\n",
    "from llama_index.query_engine.base_query_engine import BaseQueryEngine\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "custom_cache_folder = \"C:/Users/Digital/.cache/huggingface/hub/models--thenlper--gte-large/snapshots/8cb729e8b44d9ec9d85c1cec4167ed28b43b04c2/1_Pooling/\"\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"Data\").load_data()\n",
    "\n",
    "# Create LlamaCPP instance\n",
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Create LangchainEmbedding instance\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\", cache_folder=custom_cache_folder))\n",
    "\n",
    "# Create ServiceContext\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=llm, embed_model=embed_model)\n",
    "\n",
    "# Create VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Save the query engine and vector index\n",
    "with open('query_engine.pkl', 'wb') as file:\n",
    "    pickle.dump(index.as_query_engine(), file)\n",
    "\n",
    "with open('vector_index.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)\n",
    "\n",
    "# Load index from the saved file\n",
    "with open('vector_index.pkl', 'rb') as file:\n",
    "    loaded_index = pickle.load(file)\n",
    "\n",
    "# Create query engine from loaded index\n",
    "loaded_query_engine = loaded_index.as_query_engine()\n",
    "\n",
    "# Query the loaded index\n",
    "response = loaded_query_engine.query(\"What is 67?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e2c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement llama_index.query_engine (from versions: none)\n",
      "ERROR: No matching distribution found for llama_index.query_engine\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index.query_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1117ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
